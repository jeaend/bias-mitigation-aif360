{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9ec5e7",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "- for 'sex' and 'race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "\n",
    "# Internal Functions\n",
    "from src.data_preprocessing import preprocessing_adult\n",
    "\n",
    "# pd settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# data\n",
    "from aif360.datasets import AdultDataset, BinaryLabelDataset\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluation\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35466a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'sex'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad = AdultDataset(\n",
    "    protected_attribute_names=[protected],\n",
    "    privileged_classes=[['Male']],\n",
    "    categorical_features=[\n",
    "        'workclass',\n",
    "        'education',\n",
    "        'marital-status',\n",
    "        'occupation',\n",
    "        'relationship',\n",
    "        'race',\n",
    "        'native-country'\n",
    "    ],\n",
    "    features_to_drop=[''],\n",
    "    instance_weights_name='fnlwgt',\n",
    "    custom_preprocessing=preprocessing_adult,\n",
    "    na_values=[]\n",
    ")\n",
    "# 2) Build a single DataFrame (features + label) \n",
    "df = pd.DataFrame(ad.features, columns=ad.feature_names)\n",
    "df['label'] = ad.labels.ravel()\n",
    "\n",
    "# 3) Stratified splitter for 25 runs, 80/20 split\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df  = df.iloc[test_idx]\n",
    "\n",
    "    # 4) Scale features\n",
    "    scaler = StandardScaler().fit(train_df[ad.feature_names])\n",
    "    X_train = scaler.transform(train_df[ad.feature_names])\n",
    "    X_test  = scaler.transform(test_df[ad.feature_names])\n",
    "    y_train = train_df['label']\n",
    "    y_test  = test_df['label']\n",
    "\n",
    "    # 5) Train & predict\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # 6) Wrap into AIF360 datasets\n",
    "    test_orig = BinaryLabelDataset(\n",
    "        favorable_label=1.0, unfavorable_label=0.0,\n",
    "        df=test_df,\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=[protected],\n",
    "        privileged_protected_attributes=[[privileged_value]],\n",
    "        unprivileged_protected_attributes=[[unprivileged_value]]\n",
    "    )\n",
    "    test_pred = test_orig.copy(deepcopy=True)\n",
    "    test_pred.labels = y_pred.reshape(-1,1)\n",
    "\n",
    "    # 7a) Outcome‐level metrics\n",
    "    bldm = BinaryLabelDatasetMetric(\n",
    "        test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    spd = bldm.statistical_parity_difference()\n",
    "    di  = bldm.disparate_impact()\n",
    "\n",
    "    # 7b) Error-Based metrics\n",
    "    cls = ClassificationMetric(\n",
    "        test_orig, test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    eod = cls.equal_opportunity_difference()\n",
    "    aod = cls.average_odds_difference()\n",
    "\n",
    "    # 9) Compute performance metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "\n",
    "    # 10) Append to results\n",
    "    results.append({\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'statistical_parity_difference': spd,\n",
    "        'disparate_impact': di,\n",
    "        'equal_opportunity_difference': eod,\n",
    "        'average_odds_difference': aod\n",
    "    })\n",
    "\n",
    "# 11) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c84cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'race'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad = AdultDataset(\n",
    "    protected_attribute_names=[protected],\n",
    "    privileged_classes=[['White']],\n",
    "    categorical_features=[\n",
    "        'workclass',\n",
    "        'education',\n",
    "        'marital-status',\n",
    "        'occupation',\n",
    "        'relationship',\n",
    "        'sex',\n",
    "        'native-country'\n",
    "    ],\n",
    "    features_to_drop=[''],\n",
    "    instance_weights_name='fnlwgt',\n",
    "    custom_preprocessing=preprocessing_adult,\n",
    "    na_values=[]\n",
    ")\n",
    "# 2) Build a single DataFrame (features + label) \n",
    "df = pd.DataFrame(ad.features, columns=ad.feature_names)\n",
    "df['label'] = ad.labels.ravel()\n",
    "\n",
    "# 3) Stratified splitter for 25 runs, 80/20 split\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df  = df.iloc[test_idx]\n",
    "\n",
    "    # 4) Scale features\n",
    "    scaler = StandardScaler().fit(train_df[ad.feature_names])\n",
    "    X_train = scaler.transform(train_df[ad.feature_names])\n",
    "    X_test  = scaler.transform(test_df[ad.feature_names])\n",
    "    y_train = train_df['label']\n",
    "    y_test  = test_df['label']\n",
    "\n",
    "    # 5) Train & predict\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # 6) Wrap into AIF360 datasets\n",
    "    test_orig = BinaryLabelDataset(\n",
    "        favorable_label=1.0, unfavorable_label=0.0,\n",
    "        df=test_df,\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=[protected],\n",
    "        privileged_protected_attributes=[[privileged_value]],\n",
    "        unprivileged_protected_attributes=[[unprivileged_value]]\n",
    "    )\n",
    "    test_pred = test_orig.copy(deepcopy=True)\n",
    "    test_pred.labels = y_pred.reshape(-1,1)\n",
    "\n",
    "    # 7a) Outcome‐level metrics\n",
    "    bldm = BinaryLabelDatasetMetric(\n",
    "        test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    spd = bldm.statistical_parity_difference()\n",
    "    di  = bldm.disparate_impact()\n",
    "\n",
    "    # 7b) Error-Based metrics\n",
    "    cls = ClassificationMetric(\n",
    "        test_orig, test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    eod = cls.equal_opportunity_difference()\n",
    "    aod = cls.average_odds_difference()\n",
    "\n",
    "    # 9) Compute performance metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "\n",
    "    # 10) Append to results\n",
    "    results.append({\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'statistical_parity_difference': spd,\n",
    "        'disparate_impact': di,\n",
    "        'equal_opportunity_difference': eod,\n",
    "        'average_odds_difference': aod\n",
    "    })\n",
    "\n",
    "# 11) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f537fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d502e",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "### Refactored code, leaving base iteration intact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1eff2",
   "metadata": {},
   "source": [
    "#### Evaluation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fe67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    test_df,\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    protected: str,\n",
    "    privileged_value: float = 1.0,\n",
    "    unprivileged_value: float = 0.0\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Wraps dataframe + predictions into AIF360 datasets structure and returns a dict of:\n",
    "      - accuracy, f1_score,\n",
    "      - statistical_parity_difference, disparate_impact,\n",
    "      - equal_opportunity_difference, average_odds_difference\n",
    "    \"\"\"\n",
    "    # 1) Original & predicted BinaryLabelDataset\n",
    "    orig = BinaryLabelDataset(\n",
    "        favorable_label=1.0, unfavorable_label=0.0,\n",
    "        df=test_df, label_names=['label'],\n",
    "        protected_attribute_names=[protected],\n",
    "        privileged_protected_attributes=[[privileged_value]],\n",
    "        unprivileged_protected_attributes=[[unprivileged_value]]\n",
    "    )\n",
    "    pred = orig.copy(deepcopy=True)\n",
    "    pred.labels = y_pred.reshape(-1, 1)\n",
    "\n",
    "    # 2) Outcome‐based metrics\n",
    "    bldm = BinaryLabelDatasetMetric(\n",
    "        pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    spd = bldm.statistical_parity_difference()\n",
    "    di  = bldm.disparate_impact()\n",
    "\n",
    "    # 3) Error‐based metrics\n",
    "    cls = ClassificationMetric(\n",
    "        orig, pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    eod = cls.equal_opportunity_difference()\n",
    "    aod = cls.average_odds_difference()\n",
    "\n",
    "    # 4) Performance metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'accuracy':                      acc,\n",
    "        'f1_score':                      f1,\n",
    "        'statistical_parity_difference': spd,\n",
    "        'disparate_impact':              di,\n",
    "        'equal_opportunity_difference':  eod,\n",
    "        'average_odds_difference':       aod\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df87e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'sex'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad = AdultDataset(\n",
    "    protected_attribute_names=[protected],\n",
    "    privileged_classes=[['Male']],\n",
    "    categorical_features=[\n",
    "        'workclass',\n",
    "        'education',\n",
    "        'marital-status',\n",
    "        'occupation',\n",
    "        'relationship',\n",
    "        'race',\n",
    "        'native-country'\n",
    "    ],\n",
    "    features_to_drop=[''],\n",
    "    instance_weights_name='fnlwgt',\n",
    "    custom_preprocessing=preprocessing_adult,\n",
    "    na_values=[]\n",
    ")\n",
    "\n",
    "# 2) Build a single DataFrame (features + label) \n",
    "df = pd.DataFrame(ad.features, columns=ad.feature_names)\n",
    "df['label'] = ad.labels.ravel()\n",
    "\n",
    "# 3) Stratified splitter for 25 runs, 80/20 split\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df  = df.iloc[test_idx]\n",
    "\n",
    "    # 4) Scale features\n",
    "    scaler = StandardScaler().fit(train_df[ad.feature_names])\n",
    "    X_train = scaler.transform(train_df[ad.feature_names])\n",
    "    X_test  = scaler.transform(test_df[ad.feature_names])\n",
    "    y_train = train_df['label']\n",
    "    y_test  = test_df['label']\n",
    "\n",
    "    # 5) Train & Predict\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # 6) Evaluate\n",
    "    evaluation_metrics = compute_metrics(\n",
    "        test_df,\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        protected=protected,            \n",
    "        privileged_value=privileged_value,\n",
    "        unprivileged_value=unprivileged_value\n",
    "    )\n",
    "    results.append(evaluation_metrics)\n",
    "\n",
    "# 11) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c8e64",
   "metadata": {},
   "source": [
    "#### Load Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessing import preprocessing_adult\n",
    "\n",
    "def load_adult_sex():\n",
    "    ad = AdultDataset(\n",
    "        protected_attribute_names=['sex'],\n",
    "        privileged_classes=[['Male']],\n",
    "        categorical_features=[\n",
    "            'workclass',\n",
    "            'education',\n",
    "            'marital-status',\n",
    "            'occupation',\n",
    "            'relationship',\n",
    "            'race',\n",
    "            'native-country'\n",
    "        ],\n",
    "        features_to_drop=[''],\n",
    "        instance_weights_name='fnlwgt',\n",
    "        custom_preprocessing=preprocessing_adult,\n",
    "        na_values=[]\n",
    "    )\n",
    "\n",
    "    # 2) Build a single DataFrame (features + label) \n",
    "    df = pd.DataFrame(ad.features, columns=ad.feature_names)\n",
    "    df['label'] = ad.labels.ravel()\n",
    "    \n",
    "    return ad, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd143c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'sex'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad, df = load_adult_sex()\n",
    "\n",
    "# 3) Stratified splitter for 25 runs, 80/20 split\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df  = df.iloc[test_idx]\n",
    "\n",
    "    # 4) Scale features\n",
    "    scaler = StandardScaler().fit(train_df[ad.feature_names])\n",
    "    X_train = scaler.transform(train_df[ad.feature_names])\n",
    "    X_test  = scaler.transform(test_df[ad.feature_names])\n",
    "    y_train = train_df['label']\n",
    "    y_test  = test_df['label']\n",
    "\n",
    "    # 5) Train & Predict\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # 6) Evaluate\n",
    "    evaluation_metrics = compute_metrics(\n",
    "        test_df,\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        protected=protected,            \n",
    "        privileged_value=privileged_value,\n",
    "        unprivileged_value=unprivileged_value\n",
    "    )\n",
    "    results.append(evaluation_metrics)\n",
    "\n",
    "# 11) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e8795",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77addce9",
   "metadata": {},
   "source": [
    "#### Model training extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_default_model_pipeline():\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf',    LogisticRegression(solver='liblinear'))\n",
    "    ])\n",
    "\n",
    "def train_and_predict(df, feature_cols, train_idx, test_idx, pipeline=None):\n",
    "    if pipeline is None:\n",
    "        pipeline = get_default_model_pipeline()  \n",
    "\n",
    "    # split + extract \n",
    "    X_train = df.iloc[train_idx][feature_cols]\n",
    "    y_train = df.iloc[train_idx]['label']\n",
    "    X_test  = df.iloc[test_idx][feature_cols]\n",
    "    y_test  = df.iloc[test_idx]['label']\n",
    "\n",
    "    # fit & predict\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # return the test‐DataFrame for metrics\n",
    "    test_df = df.iloc[test_idx]\n",
    "    return test_df, y_test, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497383a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'sex'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad, df = load_adult_sex()\n",
    "feature_cols = ad.feature_names\n",
    "\n",
    "# 2) Run experiment, Evaluate\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    test_df, y_test, y_pred = train_and_predict(\n",
    "        df, feature_cols, train_idx, test_idx\n",
    "    )\n",
    "    m = compute_metrics(test_df, y_test, y_pred, protected, privileged_value, unprivileged_value)\n",
    "    results.append(m)\n",
    "\n",
    "# 3) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383847dc",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------\n",
    "### Run from here to check refactored functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf55566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from src.data_loading import load_adult_sex\n",
    "from src.data_loading import load_adult_race\n",
    "from src.modeling import train_and_predict\n",
    "from src.metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62b6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'sex'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad, df = load_adult_sex()\n",
    "feature_cols = ad.feature_names\n",
    "\n",
    "# 2) Run experiment, Evaluate\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    test_df, y_test, y_pred = train_and_predict(\n",
    "        df, feature_cols, train_idx, test_idx\n",
    "    )\n",
    "    m = compute_metrics(test_df, y_test, y_pred, protected, privileged_value, unprivileged_value)\n",
    "    results.append(m)\n",
    "\n",
    "# 3) Aggregate results\n",
    "adult_sex_metrics = pd.DataFrame(results)\n",
    "adult_sex_metrics_agg = adult_sex_metrics.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed15882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'race'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad, df = load_adult_race()\n",
    "feature_cols = ad.feature_names\n",
    "\n",
    "# 2) Run experiment, Evaluate\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    test_df, y_test, y_pred = train_and_predict(\n",
    "        df, feature_cols, train_idx, test_idx\n",
    "    )\n",
    "    m = compute_metrics(test_df, y_test, y_pred, protected, privileged_value, unprivileged_value)\n",
    "    results.append(m)\n",
    "\n",
    "# 3) Aggregate results\n",
    "adult_race_metrics = pd.DataFrame(results)\n",
    "adult_race_metrics_agg = adult_race_metrics.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0dacb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>SPD</th>\n",
       "      <th>DI</th>\n",
       "      <th>EOD</th>\n",
       "      <th>AOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.850875</td>\n",
       "      <td>0.657510</td>\n",
       "      <td>-0.182034</td>\n",
       "      <td>0.290624</td>\n",
       "      <td>-0.119800</td>\n",
       "      <td>-0.099392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.016677</td>\n",
       "      <td>0.025528</td>\n",
       "      <td>0.013348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy  f1_score       SPD        DI       EOD       AOD\n",
       "mean  0.850875  0.657510 -0.182034  0.290624 -0.119800 -0.099392\n",
       "std   0.002878  0.005878  0.006366  0.016677  0.025528  0.013348"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_sex_metrics_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d5e0257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>SPD</th>\n",
       "      <th>DI</th>\n",
       "      <th>EOD</th>\n",
       "      <th>AOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.850871</td>\n",
       "      <td>0.657504</td>\n",
       "      <td>-0.095198</td>\n",
       "      <td>0.546383</td>\n",
       "      <td>-0.080326</td>\n",
       "      <td>-0.057502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.005256</td>\n",
       "      <td>0.023411</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>0.012064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy  f1_score       SPD        DI       EOD       AOD\n",
       "mean  0.850871  0.657504 -0.095198  0.546383 -0.080326 -0.057502\n",
       "std   0.002882  0.005880  0.005256  0.023411  0.023949  0.012064"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_race_metrics_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091d338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
