{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9ec5e7",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "- for 'sex' and 'race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4930be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c74a145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "\n",
    "# Internal Functions\n",
    "from src.data_preprocessing import preprocessing_adult\n",
    "\n",
    "# pd settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# data\n",
    "from aif360.datasets import AdultDataset, BinaryLabelDataset\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluation\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35466a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'sex'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad = AdultDataset(\n",
    "    protected_attribute_names=[protected],\n",
    "    privileged_classes=[['Male']],\n",
    "    categorical_features=[\n",
    "        'workclass',\n",
    "        'education',\n",
    "        'marital-status',\n",
    "        'occupation',\n",
    "        'relationship',\n",
    "        'race',\n",
    "        'native-country'\n",
    "    ],\n",
    "    features_to_drop=[''],\n",
    "    instance_weights_name='fnlwgt',\n",
    "    custom_preprocessing=preprocessing_adult,\n",
    "    na_values=[]\n",
    ")\n",
    "# 2) Build a single DataFrame (features + label) \n",
    "df = pd.DataFrame(ad.features, columns=ad.feature_names)\n",
    "df['label'] = ad.labels.ravel()\n",
    "\n",
    "# 3) Stratified splitter for 25 runs, 80/20 split\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df  = df.iloc[test_idx]\n",
    "\n",
    "    # 4) Scale features\n",
    "    scaler = StandardScaler().fit(train_df[ad.feature_names])\n",
    "    X_train = scaler.transform(train_df[ad.feature_names])\n",
    "    X_test  = scaler.transform(test_df[ad.feature_names])\n",
    "    y_train = train_df['label']\n",
    "    y_test  = test_df['label']\n",
    "\n",
    "    # 5) Train & predict\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # 6) Wrap into AIF360 datasets\n",
    "    test_orig = BinaryLabelDataset(\n",
    "        favorable_label=1.0, unfavorable_label=0.0,\n",
    "        df=test_df,\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=[protected],\n",
    "        privileged_protected_attributes=[[privileged_value]],\n",
    "        unprivileged_protected_attributes=[[unprivileged_value]]\n",
    "    )\n",
    "    test_pred = test_orig.copy(deepcopy=True)\n",
    "    test_pred.labels = y_pred.reshape(-1,1)\n",
    "\n",
    "    # 7a) Outcome‐level metrics\n",
    "    bldm = BinaryLabelDatasetMetric(\n",
    "        test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    spd = bldm.statistical_parity_difference()\n",
    "    di  = bldm.disparate_impact()\n",
    "\n",
    "    # 7b) Error-Based metrics\n",
    "    cls = ClassificationMetric(\n",
    "        test_orig, test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    eod = cls.equal_opportunity_difference()\n",
    "    aod = cls.average_odds_difference()\n",
    "\n",
    "    # 9) Compute performance metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "\n",
    "    # 10) Append to results\n",
    "    results.append({\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'statistical_parity_difference': spd,\n",
    "        'disparate_impact': di,\n",
    "        'equal_opportunity_difference': eod,\n",
    "        'average_odds_difference': aod\n",
    "    })\n",
    "\n",
    "# 11) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c84cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'race'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad = AdultDataset(\n",
    "    protected_attribute_names=[protected],\n",
    "    privileged_classes=[['White']],\n",
    "    categorical_features=[\n",
    "        'workclass',\n",
    "        'education',\n",
    "        'marital-status',\n",
    "        'occupation',\n",
    "        'relationship',\n",
    "        'sex',\n",
    "        'native-country'\n",
    "    ],\n",
    "    features_to_drop=[''],\n",
    "    instance_weights_name='fnlwgt',\n",
    "    custom_preprocessing=preprocessing_adult,\n",
    "    na_values=[]\n",
    ")\n",
    "# 2) Build a single DataFrame (features + label) \n",
    "df = pd.DataFrame(ad.features, columns=ad.feature_names)\n",
    "df['label'] = ad.labels.ravel()\n",
    "\n",
    "# 3) Stratified splitter for 25 runs, 80/20 split\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df  = df.iloc[test_idx]\n",
    "\n",
    "    # 4) Scale features\n",
    "    scaler = StandardScaler().fit(train_df[ad.feature_names])\n",
    "    X_train = scaler.transform(train_df[ad.feature_names])\n",
    "    X_test  = scaler.transform(test_df[ad.feature_names])\n",
    "    y_train = train_df['label']\n",
    "    y_test  = test_df['label']\n",
    "\n",
    "    # 5) Train & predict\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # 6) Wrap into AIF360 datasets\n",
    "    test_orig = BinaryLabelDataset(\n",
    "        favorable_label=1.0, unfavorable_label=0.0,\n",
    "        df=test_df,\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=[protected],\n",
    "        privileged_protected_attributes=[[privileged_value]],\n",
    "        unprivileged_protected_attributes=[[unprivileged_value]]\n",
    "    )\n",
    "    test_pred = test_orig.copy(deepcopy=True)\n",
    "    test_pred.labels = y_pred.reshape(-1,1)\n",
    "\n",
    "    # 7a) Outcome‐level metrics\n",
    "    bldm = BinaryLabelDatasetMetric(\n",
    "        test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    spd = bldm.statistical_parity_difference()\n",
    "    di  = bldm.disparate_impact()\n",
    "\n",
    "    # 7b) Error-Based metrics\n",
    "    cls = ClassificationMetric(\n",
    "        test_orig, test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    eod = cls.equal_opportunity_difference()\n",
    "    aod = cls.average_odds_difference()\n",
    "\n",
    "    # 9) Compute performance metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "\n",
    "    # 10) Append to results\n",
    "    results.append({\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'statistical_parity_difference': spd,\n",
    "        'disparate_impact': di,\n",
    "        'equal_opportunity_difference': eod,\n",
    "        'average_odds_difference': aod\n",
    "    })\n",
    "\n",
    "# 11) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f537fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d502e",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "### Refactored code, leaving base iteration intact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1eff2",
   "metadata": {},
   "source": [
    "#### Evaluation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fe67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    test_df,\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    protected: str,\n",
    "    privileged_value: float = 1.0,\n",
    "    unprivileged_value: float = 0.0\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Wraps dataframe + predictions into AIF360 datasets structure and returns a dict of:\n",
    "      - accuracy, f1_score,\n",
    "      - statistical_parity_difference, disparate_impact,\n",
    "      - equal_opportunity_difference, average_odds_difference\n",
    "    \"\"\"\n",
    "    # 1) Original & predicted BinaryLabelDataset\n",
    "    orig = BinaryLabelDataset(\n",
    "        favorable_label=1.0, unfavorable_label=0.0,\n",
    "        df=test_df, label_names=['label'],\n",
    "        protected_attribute_names=[protected],\n",
    "        privileged_protected_attributes=[[privileged_value]],\n",
    "        unprivileged_protected_attributes=[[unprivileged_value]]\n",
    "    )\n",
    "    pred = orig.copy(deepcopy=True)\n",
    "    pred.labels = y_pred.reshape(-1, 1)\n",
    "\n",
    "    # 2) Outcome‐based metrics\n",
    "    bldm = BinaryLabelDatasetMetric(\n",
    "        pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    spd = bldm.statistical_parity_difference()\n",
    "    di  = bldm.disparate_impact()\n",
    "\n",
    "    # 3) Error‐based metrics\n",
    "    cls = ClassificationMetric(\n",
    "        orig, pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    eod = cls.equal_opportunity_difference()\n",
    "    aod = cls.average_odds_difference()\n",
    "\n",
    "    # 4) Performance metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'accuracy':                      acc,\n",
    "        'f1_score':                      f1,\n",
    "        'statistical_parity_difference': spd,\n",
    "        'disparate_impact':              di,\n",
    "        'equal_opportunity_difference':  eod,\n",
    "        'average_odds_difference':       aod\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df87e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'sex'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad = AdultDataset(\n",
    "    protected_attribute_names=[protected],\n",
    "    privileged_classes=[['Male']],\n",
    "    categorical_features=[\n",
    "        'workclass',\n",
    "        'education',\n",
    "        'marital-status',\n",
    "        'occupation',\n",
    "        'relationship',\n",
    "        'race',\n",
    "        'native-country'\n",
    "    ],\n",
    "    features_to_drop=[''],\n",
    "    instance_weights_name='fnlwgt',\n",
    "    custom_preprocessing=preprocessing_adult,\n",
    "    na_values=[]\n",
    ")\n",
    "\n",
    "# 2) Build a single DataFrame (features + label) \n",
    "df = pd.DataFrame(ad.features, columns=ad.feature_names)\n",
    "df['label'] = ad.labels.ravel()\n",
    "\n",
    "# 3) Stratified splitter for 25 runs, 80/20 split\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df  = df.iloc[test_idx]\n",
    "\n",
    "    # 4) Scale features\n",
    "    scaler = StandardScaler().fit(train_df[ad.feature_names])\n",
    "    X_train = scaler.transform(train_df[ad.feature_names])\n",
    "    X_test  = scaler.transform(test_df[ad.feature_names])\n",
    "    y_train = train_df['label']\n",
    "    y_test  = test_df['label']\n",
    "\n",
    "    # 5) Train & Predict\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # 6) Evaluate\n",
    "    evaluation_metrics = compute_metrics(\n",
    "        test_df,\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        protected=protected,            \n",
    "        privileged_value=privileged_value,\n",
    "        unprivileged_value=unprivileged_value\n",
    "    )\n",
    "    results.append(evaluation_metrics)\n",
    "\n",
    "# 11) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c8e64",
   "metadata": {},
   "source": [
    "#### Load Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessing import preprocessing_adult\n",
    "\n",
    "def load_adult_sex():\n",
    "    ad = AdultDataset(\n",
    "        protected_attribute_names=['sex'],\n",
    "        privileged_classes=[['Male']],\n",
    "        categorical_features=[\n",
    "            'workclass',\n",
    "            'education',\n",
    "            'marital-status',\n",
    "            'occupation',\n",
    "            'relationship',\n",
    "            'race',\n",
    "            'native-country'\n",
    "        ],\n",
    "        features_to_drop=[''],\n",
    "        instance_weights_name='fnlwgt',\n",
    "        custom_preprocessing=preprocessing_adult,\n",
    "        na_values=[]\n",
    "    )\n",
    "\n",
    "    # 2) Build a single DataFrame (features + label) \n",
    "    df = pd.DataFrame(ad.features, columns=ad.feature_names)\n",
    "    df['label'] = ad.labels.ravel()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30700edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import compute_metrics\n",
    "from src.data_loading import load_adult_sex\n",
    "\n",
    "# 1) Retrieve data\n",
    "protected = 'sex'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "df = load_adult_sex()\n",
    "\n",
    "# 3) Stratified splitter for 25 runs, 80/20 split\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df  = df.iloc[test_idx]\n",
    "\n",
    "    # 4) Scale features\n",
    "    scaler = StandardScaler().fit(train_df[ad.feature_names])\n",
    "    X_train = scaler.transform(train_df[ad.feature_names])\n",
    "    X_test  = scaler.transform(test_df[ad.feature_names])\n",
    "    y_train = train_df['label']\n",
    "    y_test  = test_df['label']\n",
    "\n",
    "    # 5) Train & Predict\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # 6) Evaluate\n",
    "    evaluation_metrics = compute_metrics(\n",
    "        test_df,\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        protected=protected,            \n",
    "        privileged_value=privileged_value,\n",
    "        unprivileged_value=unprivileged_value\n",
    "    )\n",
    "    results.append(evaluation_metrics)\n",
    "\n",
    "# 11) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e8795",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65bb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
