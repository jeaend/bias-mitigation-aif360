{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9ec5e7",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "- for 'sex' and 'race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74a145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "\n",
    "# Internal Functions\n",
    "from src.data_preprocessing import preprocessing_adult\n",
    "\n",
    "# pd settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# data\n",
    "from aif360.datasets import AdultDataset, BinaryLabelDataset\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluation\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35466a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'sex'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad = AdultDataset(\n",
    "    protected_attribute_names=[protected],\n",
    "    privileged_classes=[['Male']],\n",
    "    categorical_features=[\n",
    "        'workclass',\n",
    "        'education',\n",
    "        'marital-status',\n",
    "        'occupation',\n",
    "        'relationship',\n",
    "        'race',\n",
    "        'native-country'\n",
    "    ],\n",
    "    features_to_drop=[''],\n",
    "    instance_weights_name='fnlwgt',\n",
    "    custom_preprocessing=preprocessing_adult,\n",
    "    na_values=[]\n",
    ")\n",
    "# 2) Build a single DataFrame (features + label) \n",
    "df = pd.DataFrame(ad.features, columns=ad.feature_names)\n",
    "df['label'] = ad.labels.ravel()\n",
    "\n",
    "# 3) Stratified splitter for 25 runs, 80/20 split\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df  = df.iloc[test_idx]\n",
    "\n",
    "    # 4) Scale features\n",
    "    scaler = StandardScaler().fit(train_df[ad.feature_names])\n",
    "    X_train = scaler.transform(train_df[ad.feature_names])\n",
    "    X_test  = scaler.transform(test_df[ad.feature_names])\n",
    "    y_train = train_df['label']\n",
    "    y_test  = test_df['label']\n",
    "\n",
    "    # 5) Train & predict\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # 6) Wrap into AIF360 datasets\n",
    "    test_orig = BinaryLabelDataset(\n",
    "        favorable_label=1.0, unfavorable_label=0.0,\n",
    "        df=test_df,\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=[protected],\n",
    "        privileged_protected_attributes=[[privileged_value]],\n",
    "        unprivileged_protected_attributes=[[unprivileged_value]]\n",
    "    )\n",
    "    test_pred = test_orig.copy(deepcopy=True)\n",
    "    test_pred.labels = y_pred.reshape(-1,1)\n",
    "\n",
    "    # 7a) Outcome‐level metrics\n",
    "    bldm = BinaryLabelDatasetMetric(\n",
    "        test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    spd = bldm.statistical_parity_difference()\n",
    "    di  = bldm.disparate_impact()\n",
    "\n",
    "    # 7b) Error-Based metrics\n",
    "    cls = ClassificationMetric(\n",
    "        test_orig, test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    eod = cls.equal_opportunity_difference()\n",
    "    aod = cls.average_odds_difference()\n",
    "\n",
    "    # 9) Compute performance metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "\n",
    "    # 10) Append to results\n",
    "    results.append({\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'statistical_parity_difference': spd,\n",
    "        'disparate_impact': di,\n",
    "        'equal_opportunity_difference': eod,\n",
    "        'average_odds_difference': aod\n",
    "    })\n",
    "\n",
    "# 11) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff81820c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>statistical_parity_difference</th>\n",
       "      <th>disparate_impact</th>\n",
       "      <th>equal_opportunity_difference</th>\n",
       "      <th>average_odds_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.850875</td>\n",
       "      <td>0.657510</td>\n",
       "      <td>-0.182034</td>\n",
       "      <td>0.290624</td>\n",
       "      <td>-0.119800</td>\n",
       "      <td>-0.099392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.016677</td>\n",
       "      <td>0.025528</td>\n",
       "      <td>0.013348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy  f1_score  statistical_parity_difference  disparate_impact  \\\n",
       "mean  0.850875  0.657510                      -0.182034          0.290624   \n",
       "std   0.002878  0.005878                       0.006366          0.016677   \n",
       "\n",
       "      equal_opportunity_difference  average_odds_difference  \n",
       "mean                     -0.119800                -0.099392  \n",
       "std                       0.025528                 0.013348  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83c84cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrieve data\n",
    "protected = 'race'\n",
    "privileged_value   = 1.0\n",
    "unprivileged_value = 0.0\n",
    "\n",
    "ad = AdultDataset(\n",
    "    protected_attribute_names=[protected],\n",
    "    privileged_classes=[['White']],\n",
    "    categorical_features=[\n",
    "        'workclass',\n",
    "        'education',\n",
    "        'marital-status',\n",
    "        'occupation',\n",
    "        'relationship',\n",
    "        'sex',\n",
    "        'native-country'\n",
    "    ],\n",
    "    features_to_drop=[''],\n",
    "    instance_weights_name='fnlwgt',\n",
    "    custom_preprocessing=preprocessing_adult,\n",
    "    na_values=[]\n",
    ")\n",
    "# 2) Build a single DataFrame (features + label) \n",
    "df = pd.DataFrame(ad.features, columns=ad.feature_names)\n",
    "df['label'] = ad.labels.ravel()\n",
    "\n",
    "# 3) Stratified splitter for 25 runs, 80/20 split\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for train_idx, test_idx in sss.split(df, df['label']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df  = df.iloc[test_idx]\n",
    "\n",
    "    # 4) Scale features\n",
    "    scaler = StandardScaler().fit(train_df[ad.feature_names])\n",
    "    X_train = scaler.transform(train_df[ad.feature_names])\n",
    "    X_test  = scaler.transform(test_df[ad.feature_names])\n",
    "    y_train = train_df['label']\n",
    "    y_test  = test_df['label']\n",
    "\n",
    "    # 5) Train & predict\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # 6) Wrap into AIF360 datasets\n",
    "    test_orig = BinaryLabelDataset(\n",
    "        favorable_label=1.0, unfavorable_label=0.0,\n",
    "        df=test_df,\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=[protected],\n",
    "        privileged_protected_attributes=[[privileged_value]],\n",
    "        unprivileged_protected_attributes=[[unprivileged_value]]\n",
    "    )\n",
    "    test_pred = test_orig.copy(deepcopy=True)\n",
    "    test_pred.labels = y_pred.reshape(-1,1)\n",
    "\n",
    "    # 7a) Outcome‐level metrics\n",
    "    bldm = BinaryLabelDatasetMetric(\n",
    "        test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    spd = bldm.statistical_parity_difference()\n",
    "    di  = bldm.disparate_impact()\n",
    "\n",
    "    # 7b) Error-Based metrics\n",
    "    cls = ClassificationMetric(\n",
    "        test_orig, test_pred,\n",
    "        privileged_groups=[{protected: privileged_value}],\n",
    "        unprivileged_groups=[{protected: unprivileged_value}]\n",
    "    )\n",
    "    eod = cls.equal_opportunity_difference()\n",
    "    aod = cls.average_odds_difference()\n",
    "\n",
    "    # 9) Compute performance metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "\n",
    "    # 10) Append to results\n",
    "    results.append({\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'statistical_parity_difference': spd,\n",
    "        'disparate_impact': di,\n",
    "        'equal_opportunity_difference': eod,\n",
    "        'average_odds_difference': aod\n",
    "    })\n",
    "\n",
    "# 11) Aggregate results\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df_agg = metrics_df.agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3f537fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>statistical_parity_difference</th>\n",
       "      <th>disparate_impact</th>\n",
       "      <th>equal_opportunity_difference</th>\n",
       "      <th>average_odds_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.850871</td>\n",
       "      <td>0.657504</td>\n",
       "      <td>-0.095198</td>\n",
       "      <td>0.546383</td>\n",
       "      <td>-0.080326</td>\n",
       "      <td>-0.057502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.005256</td>\n",
       "      <td>0.023411</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>0.012064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy  f1_score  statistical_parity_difference  disparate_impact  \\\n",
       "mean  0.850871  0.657504                      -0.095198          0.546383   \n",
       "std   0.002882  0.005880                       0.005256          0.023411   \n",
       "\n",
       "      equal_opportunity_difference  average_odds_difference  \n",
       "mean                     -0.080326                -0.057502  \n",
       "std                       0.023949                 0.012064  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bad64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
